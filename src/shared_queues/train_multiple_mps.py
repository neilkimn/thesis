import ads3 as ads3
import torch
import torch.backends.cudnn as cudnn
import torchvision
import torchvision.transforms as transforms
import torchvision.models as models
import torchvision.datasets as datasets

import torch.multiprocessing as mp
from torch.multiprocessing import Process, JoinableQueue
from torch.utils import data as D
from torch.autograd import Variable

from pathlib import Path
import shutil
import argparse
import random
import time
import os

from shared.util import MyQueue, Logger, write_debug_indices, Counter, get_transformations
from shared.dataset import CarDataset, DatasetFromSubset
from shared_queues.trainer import ProcTrainer
from shared.mps import MPSWeights

INPUT_SIZE = 224
data_path = Path(os.environ["DATA_PATH"])

from PIL import ImageFile
ImageFile.LOAD_TRUNCATED_IMAGES = True

model_names = sorted(name for name in models.__dict__
    if name.islower() and not name.startswith("__")
    and callable(models.__dict__[name]))
my_datasets = ["compcars", "imagenet", "imagenet64x64"]

parser = argparse.ArgumentParser()
parser.add_argument('--batch-size', type=int, default=80)
parser.add_argument('--num-processes', type=int, default=2)
parser.add_argument('--training-workers', type=int, default=1)
parser.add_argument('--validation-workers', type=int, default=1)
parser.add_argument('--prefetch-factor', type=int, default=1)
parser.add_argument('--seed', type=int, default=None)
parser.add_argument('--epochs', type=int, default=5)
parser.add_argument('--log-interval', type=int, default=10, metavar='N',
                    help='how many batches to wait before logging training status')
parser.add_argument('-a','--arch', nargs='+', metavar='ARCH', help='model architecture: ' +
                        ' | '.join(model_names) +
                        ' (default: resnet18)', default='resnet18')
parser.add_argument('--dataset', metavar='DATASET', help='dataset: ' +
                        ' | '.join(my_datasets) +
                        ' (default: compcars)', default='compcars')
parser.add_argument('--debug_data_dir', metavar='DIR', nargs='?', default='',
                    help='path to store data generated by dataloader')
parser.add_argument('--overwrite_debug_data', type=int, default=1)
parser.add_argument('--log_dir', metavar='LOG_DIR', nargs='?', default='',
                    help='path to store training log')
parser.add_argument('--pretrained', nargs='+', metavar="PRETRAIN", help="Whether to pretrain a certain model")
parser.add_argument('--dummy_data', action='store_true', help="use fake data to benchmark")
parser.add_argument('--record_first_batch_time', action='store_true', help="Don't skip measuring time spent on first batch")

train_transforms = transforms.Compose(
    [
        transforms.Resize((INPUT_SIZE, INPUT_SIZE)),
        transforms.RandomVerticalFlip(p=0.5),
        transforms.RandomPerspective(p=0.5),

        transforms.RandomApply(torch.nn.ModuleList([
            transforms.ColorJitter(contrast=0.5, saturation=0.5, hue=0.5),
        ]), p=0.5),

        transforms.RandomApply(torch.nn.ModuleList([
            transforms.Grayscale(num_output_channels=3),
        ]), p=0.5),

        transforms.ToTensor(),
    ]
)

valid_transforms = transforms.Compose(
    [
        transforms.Resize((INPUT_SIZE, INPUT_SIZE)),
        transforms.ToTensor(),
    ]
)

def producer(loader, valid_loader, qs, device, args, producer_alive, optimize_mps=None):
    if args.seed:
        torch.manual_seed(args.seed)
    if args.debug_data_dir:
        if args.overwrite_debug_data:
            shutil.rmtree(args.debug_data_dir)
    debug_indices_path, debug_indices_val_path = None, None

    for epoch in range(1, args.epochs+1):
        if args.debug_data_dir:
            debug_indices_path = Path(args.debug_data_dir) / f"epoch_{epoch}" / "producer_indices.txt"
            debug_indices_path.parent.mkdir(parents=True, exist_ok=True)

        for idx, (inputs, labels) in enumerate(loader):
            indices = None
            inputs = Variable(inputs.to(device))
            labels = Variable(labels.to(device))

            if optimize_mps:
                if not optimize_mps.is_set():
                    print("breaking out of loop")
                    break

            for q in qs:
                if optimize_mps:
                    if optimize_mps.is_set():
                        q.queue.put((idx, inputs, labels, epoch, "train", indices))
                else:
                    q.queue.put((idx, inputs, labels, epoch, "train", indices))
            

            write_debug_indices(indices, debug_indices_path, args)
        if optimize_mps:
            if not optimize_mps.is_set():
                print("breaking out of loop")
                break
        # end of training for epoch, switch to eval
        if epoch > 10:
            if args.debug_data_dir:
                debug_indices_val_path = Path(args.debug_data_dir) / f"epoch_{epoch}" / "producer_val_indices.txt"
                debug_indices_val_path.parent.mkdir(parents=True, exist_ok=True)

            for idx, (inputs, labels) in enumerate(valid_loader):
                indices = None
                inputs = Variable(inputs.to(device))
                labels = Variable(labels.to(device))

                for q in qs:
                    q.queue.put((idx, inputs, labels, epoch, "valid", indices))

                write_debug_indices(indices, debug_indices_val_path, args)
        
        for q in qs:
            q.queue.put((0, None, None, epoch, "end", None))
    print("Waiting to exit producer ...")
    producer_alive.wait()


# Use this worker for setting MPS weights
def MPS_worker(q, model, mps_weights, args, mps_time, empty_MPS_queue):
    alive = mp.Event()
    alive.set()
    proc_id = os.getpid()

    if model.on_device == False:
        model.send_model()
        model.scheduler.step()
    
    convergence = False

    train_time, val_time, batch_time, items_processed = 0,0,0,0
    # Process can be terminated if we've converged
    while alive.is_set():
        start = time.time()
        idx, inputs, labels, epoch, batch_type, indices = q.queue.get()
        
        # Log batch time as MPS time to be included in actual training logs
        mps_time["batch_time"] += time.time() - start
        #batch_time = time.time() - start
        
        # If:
        #   1) We haven't converged (seen three iterations with (roughly) equal queue sizes)
        #   2) We have gone through N batches, where N corresponds to the update_interval
        #   3) We will not be restarting another of these processes afterwards
        # then we can update MPS weights
        if not convergence and idx % mps_weights.update_interval == 0:
            mps_weights.set_qsize(q.queue.qsize(), q.index, proc_id, model.name)
            mps_weights.update_weights(proc_id)

            convergence = mps_weights.get_convergence()

            # Terminate MPS weight-searching process, but only if we are not on the last started process
            # On the last process, we empty the queue and then we can kill process
            if not empty_MPS_queue.is_set():
                alive.clear()

        if batch_type == "train":
            start = time.time()
            loss = model.forward(inputs, labels)
            items_processed += len(inputs)

            # Log training time as MPS time to be included in actual training logs
            mps_time["train_time"] += time.time() - start
            #train_time = time.time() - start
            #logger.log_train_interval(idx, epoch, len(inputs), loss, items_processed, train_time, batch_time, set_mps_time=True)

        # For now, hope we dont reach this stage :D
        elif batch_type == "valid":
            val_loss, val_acc, val_correct = model.validate(inputs, labels)
            
        elif batch_type == "end":
            train_epoch_acc, train_running_corrects = model.end_epoch(args)
        
        if q.queue.empty():
            alive.clear()

        q.queue.task_done()

def worker(q, model, args, producer_alive, finished_workers, mps_time=None):
    log_path, gpu_path, pid = None, None, 0
    if model.on_device == False:
        pid = os.getpid()
        print(f"{pid}\tTraining model: {model.name}")
        model.send_model()
        model.scheduler.step()
        if args.log_dir:
            log_path, gpu_path = model.init_log(pid)

    logger = Logger(args, pid, log_path, gpu_path)
    if mps_time:
        logger.set_mps_time(mps_time)

    if not args.record_first_batch_time:
        print("Skipping recording batch time for first batch!")
    
    debug_indices_path, debug_indices_val_path = None, None

    epochs_processed = 0
    train_time, val_time, batch_time, items_processed = 0,0,0,0
    big_start = time.time()

    while True:
        start = time.time()
        idx, inputs, labels, epoch, batch_type, indices = q.queue.get()

        if args.debug_data_dir:
            debug_indices_path = Path(args.debug_data_dir) / model.name / f"epoch_{epoch}" / "indices.txt"
            debug_indices_path.parent.mkdir(parents=True, exist_ok=True)
            debug_indices_val_path = Path(args.debug_data_dir) / model.name / f"epoch_{epoch}" / "val_indices.txt"
            debug_indices_val_path.parent.mkdir(parents=True, exist_ok=True)

        if args.record_first_batch_time:
            batch_time += time.time() - start
        else:
            if idx > 0:
                batch_time += time.time() - start

        start = time.time()
        if batch_type == "train":
            write_debug_indices(indices, debug_indices_path, args)
            loss = model.forward(inputs, labels)
            items_processed += len(inputs)
            train_time += time.time() - start
            logger.log_train_interval(idx, epoch, len(inputs), loss, items_processed, train_time, batch_time)

        elif batch_type == "valid":
            write_debug_indices(indices, debug_indices_val_path, args)
            val_loss, val_acc, val_correct = model.validate(inputs, labels)
            val_time += time.time() - start
            logger.log_validation(val_loss, val_correct, val_acc, val_time)
            
        elif batch_type == "end":
            train_epoch_acc, train_running_corrects = model.end_epoch(args)
            #epoch_time = train_time + val_time + batch_time
            epoch_time = time.time() - big_start
            logger.log_write_epoch_end(epoch, epoch_time, train_epoch_acc, train_running_corrects)
            train_time, val_time, batch_time,items_processed = 0,0,0,0
            print(f"Epoch took {time.time() - big_start} seconds")
            big_start = time.time()

        q.queue.task_done()

        if batch_type == "end":
            epochs_processed += 1
            if epochs_processed == args.epochs:
                finished_workers.increment()
                if finished_workers.value() == args.num_processes:
                    producer_alive.set()
                break

if __name__ == "__main__":

    args = parser.parse_args()
    assert args.num_processes == len(args.arch)
    parent_pid = os.getpid()

    if args.seed is not None:
        print(f"Setting seed {args.seed}")
        random.seed(args.seed)
        torch.manual_seed(args.seed)
        cudnn.deterministic = True
        cudnn.benchmark = False
        print('You have chosen to seed training. '
                      'This will turn on the CUDNN deterministic setting, '
                      'which can slow down your training considerably!')
        
    mp.set_start_method("spawn", force=True)
    manager = mp.Manager()
    mps_time = manager.dict()
    mps_time["train_time"] = 0
    mps_time["batch_time"] = 0

    """Initialise dataset"""
    labels = ads3.get_labels()

    pretrain = ["imagenet" if p == "true" else None for p in args.pretrained]

    device = torch.device("cuda")

    if args.dataset == "imagenet64x64":
        INPUT_SIZE = 64
    
    train_transforms, valid_transforms = get_transformations(args.dataset, INPUT_SIZE)

    if args.dataset in ["imagenet", "imagenet64x64"]:
        traindir = os.path.join(data_path / args.dataset, 'train')
        valdir = os.path.join(data_path / args.dataset, 'val')

        train_dataset = datasets.ImageFolder(
            traindir,
            train_transforms)
        
        valid_dataset = datasets.ImageFolder(
            valdir,
            valid_transforms)

    elif args.dataset == "compcars":
        labels = ads3.get_labels()
        file_train = data_path / "compcars" / "train.txt"
        folder_images = data_path / "compcars" / "image"
        dataset = CarDataset(file_path=file_train, folder_images=folder_images, labels=labels)

        train_len = int(0.7 * len(dataset))
        valid_len = len(dataset) - train_len
        train_set, valid_set = D.random_split(dataset, lengths=[train_len, valid_len], generator=torch.Generator().manual_seed(42))

        train_dataset = DatasetFromSubset(train_set, train_transforms)
        valid_dataset = DatasetFromSubset(valid_set, valid_transforms)

    train_models = []
    for idx, arch in enumerate(args.arch):
        model = torchvision.models.__dict__[arch](pretrained=pretrain[idx])
        model.name = arch + "_pretrained" if pretrain[idx] else arch
        print(f"Model: {model.name}")
        proc_model = ProcTrainer(args, model, device)
        train_models.append(proc_model)

    MPSqueues = []
    queues = []
    max_queue_size = 1
    mps_queue_size = 20

    for idx in range(args.num_processes):
        q = JoinableQueue(maxsize=max_queue_size)
        q_mps = JoinableQueue(maxsize=mps_queue_size)

        queue = MyQueue(q, idx)
        MPSqueue = MyQueue(q_mps, idx)
        
        queues.append(queue)
        MPSqueues.append(MPSqueue)


    model_names = "_".join([model.name for model in train_models])

    mps_log_path = Path(f"/home/neni/repos/thesis/logs_debug/mps/mps_log_{model_names}_pid_{parent_pid}.csv")
    print(f"Writing MPS weight logs to {mps_log_path}")
    
    mps_weights = MPSWeights(args.num_processes, max_size=mps_queue_size, increment=4, update_interval=10, log_path=mps_log_path)


    train_loader = D.DataLoader(
        train_dataset,
        batch_size=args.batch_size,
        shuffle=True,
        num_workers=args.training_workers,
        pin_memory=False,
        prefetch_factor=args.prefetch_factor,
        persistent_workers=False,
    )

    valid_loader = D.DataLoader(
        valid_dataset,
        batch_size=args.batch_size,
        shuffle=True,
        num_workers=args.validation_workers,
        pin_memory=False,
        prefetch_factor=args.prefetch_factor,
        persistent_workers=False,
    )

    args.train_dataset_len = len(train_loader.dataset)
    args.train_loader_len = len(train_loader)

    args.valid_dataset_len = len(valid_loader.dataset)
    args.valid_loader_len = len(valid_loader)

    #optimize_mps = True
    optimize_mps = mp.Event()
    optimize_mps.set()
    MPS_producer_alive = mp.Event()

    empty_MPS_queue = mp.Event()

    producers = []
    for i in range(1):
        p = Process(target=producer, args = ((train_loader, valid_loader, MPSqueues, device, args, MPS_producer_alive, optimize_mps)))
        producers.append(p)
        p.start()

    # Measuring time for entire training
    _start = time.time()

    # Measuring time spent on MPS stuff only
    mps_start =time.time()

    # Keep starting MPS-weight searching training processes until we have converged at satisfactory MPS weights
    print("Started finding MPS thread weights...")
    
    logger = Logger(args)
    while optimize_mps.is_set():
        workers = []
        # Start training processes with default MPS percentages
        for i in range(args.num_processes):
            os.environ['CUDA_MPS_ACTIVE_THREAD_PERCENTAGE'] = str(mps_weights[i])
            p = Process(target=MPS_worker, daemon=True, args=((MPSqueues[i], train_models[i], mps_weights, args, mps_time, empty_MPS_queue)))
            workers.append(p)
            p.start()

        # Weight-search converged, stop restarting new processes
        if mps_weights.get_convergence():
            #optimize_mps = False
            optimize_mps.clear()
            empty_MPS_queue.set()

        for w in workers:
            w.join()

    MPS_producer_alive.set()
    producer_alive = mp.Event()

    # Start new producer with queues with smaller size
    producers = []
    for i in range(1):
        p = Process(target=producer, args = ((train_loader, valid_loader, queues, device, args, producer_alive)))
        producers.append(p)
        p.start()
    
    # TODO: Split this up into train, batch and misc. time. For now just add total MPS time to epoch time
    #total_mps_time = time.time() - mps_start
    #mps_time["misc_time"] = total_mps_time - sum(mps_time.values())
    mps_time = time.time() - mps_start
    #print(f"Spent {mps_time['train_time']} seconds on training, {mps_time['batch_time']} seconds on batch time and {mps_time['misc_time']} seconds on misc MPS stuff")
    print(f"Done with figuring out MPS weights, took {mps_time} seconds")

    finished_workers = Counter(0)
    workers = []
    for i in range(args.num_processes):
        print(f"Setting MPS threads to {mps_weights[i]}")
        os.environ['CUDA_MPS_ACTIVE_THREAD_PERCENTAGE'] = str(mps_weights[i])
        p = Process(target=worker, daemon=True, args=((queues[i], train_models[i], args, producer_alive, finished_workers, mps_time)))
        workers.append(p)
        p.start()

    for p in workers:
        p.join()

    print(f"Completed in {time.time() - _start} seconds")