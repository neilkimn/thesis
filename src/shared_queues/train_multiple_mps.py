import ads3 as ads3
import torch
import torch.backends.cudnn as cudnn
import torchvision
import torchvision.transforms as transforms
import torchvision.models as models
import torchvision.datasets as datasets

import torch.multiprocessing as mp
from torch.multiprocessing import Process, Queue, JoinableQueue
from torch.multiprocessing import Value, Lock, Array
from torch.utils import data as D
from torch.autograd import Variable

from pathlib import Path
import shutil
import argparse
import random
import time
import os

from collections.abc import Sequence

from shared.dataset import CarDataset, DatasetFromSubset
from shared_queues.trainer import ProcTrainer

INPUT_SIZE = 224
root = Path(os.environ["DATA_PATH"])
file_train = root / "train.txt"
folder_images = root / "image"
images_train = root / "image_train"
images_valid = root / "image_valid"

model_names = sorted(name for name in models.__dict__
    if name.islower() and not name.startswith("__")
    and callable(models.__dict__[name]))

parser = argparse.ArgumentParser()
parser.add_argument('--batch-size', type=int, default=80)
parser.add_argument('--num-processes', type=int, default=2)
parser.add_argument('--training-workers', type=int, default=1)
parser.add_argument('--validation-workers', type=int, default=1)
parser.add_argument('--prefetch-factor', type=int, default=1)
parser.add_argument('--seed', type=int, default=None)
parser.add_argument('--epochs', type=int, default=5)
parser.add_argument('--log-interval', type=int, default=10, metavar='N',
                    help='how many batches to wait before logging training status')
parser.add_argument('-a','--arch', nargs='+', metavar='ARCH', help='model architecture: ' +
                        ' | '.join(model_names) +
                        ' (default: resnet18)', default='resnet18')

parser.add_argument('--debug_data_dir', metavar='DIR', nargs='?', default='',
                    help='path to store data generated by dataloader')
parser.add_argument('--overwrite_debug_data', type=int, default=1)
parser.add_argument('--log_dir', metavar='LOG_DIR', nargs='?', default='',
                    help='path to store training log')
parser.add_argument('--pretrained', nargs='+', metavar="PRETRAIN", help="Whether to pretrain a certain model")
parser.add_argument('--dummy_data', action='store_true', help="use fake data to benchmark")
parser.add_argument('--record_first_batch_time', action='store_true', help="Don't skip measuring time spent on first batch")

train_transforms = transforms.Compose(
    [
        transforms.Resize((INPUT_SIZE, INPUT_SIZE)),
        transforms.RandomVerticalFlip(p=0.5),
        transforms.RandomPerspective(p=0.5),

        transforms.RandomApply(torch.nn.ModuleList([
            transforms.ColorJitter(contrast=0.5, saturation=0.5, hue=0.5),
        ]), p=0.5),

        transforms.RandomApply(torch.nn.ModuleList([
            transforms.Grayscale(num_output_channels=3),
        ]), p=0.5),

        transforms.ToTensor(),
    ]
)

valid_transforms = transforms.Compose(
    [
        transforms.Resize((INPUT_SIZE, INPUT_SIZE)),
        transforms.ToTensor(),
    ]
)

def write_debug_indices(indices, debug_indices_path, args):
    if args.debug_data_dir:
        with open(debug_indices_path, "a") as f:
            f.write(" ".join(list(map(str, indices.tolist()))))
            f.write("\n")


def count_parameters(model):
    return sum(p.numel() for p in model.parameters() if p.requires_grad)

def producer(loader, valid_loader, qs, device, args):
    pid = os.getpid()
    torch.manual_seed(args.seed)
    if args.debug_data_dir:
        if args.overwrite_debug_data:
            shutil.rmtree(args.debug_data_dir)
    debug_indices_path, debug_indices_val_path = None, None

    for epoch in range(1, args.epochs+1):
        if args.debug_data_dir:
            debug_indices_path = Path(args.debug_data_dir) / f"epoch_{epoch}" / "producer_indices.txt"
            debug_indices_path.parent.mkdir(parents=True, exist_ok=True)

        for idx, (inputs, labels, indices) in enumerate(loader):
            inputs = Variable(inputs.to(device))
            labels = Variable(labels.to(device))

            for q in qs:
                torch.cuda.nvtx.range_push(f"Put train batch")
                q.queue.put((idx, inputs, labels, epoch, "train", indices))
                torch.cuda.nvtx.range_pop()
            
            write_debug_indices(indices, debug_indices_path, args)

        # end of training for epoch, switch to eval
        if args.debug_data_dir:
            debug_indices_val_path = Path(args.debug_data_dir) / f"epoch_{epoch}" / "producer_val_indices.txt"
            debug_indices_val_path.parent.mkdir(parents=True, exist_ok=True)

        for idx, (inputs, labels, indices) in enumerate(valid_loader):
            inputs = Variable(inputs.to(device))
            labels = Variable(labels.to(device))

            for q in qs:
                q.queue.put((idx, inputs, labels, epoch, "valid", indices))

            write_debug_indices(indices, debug_indices_val_path, args)
        
        for q in qs:
            q.queue.put((0, None, None, epoch, "end", None))


class MaxVal(object):
    def __init__(self, initval=0):
        self.val = Value('i', initval)
        self.lock = Lock()

    def value(self):
        with self.lock:
            return self.val.value
        
    def set_value(self, value):
        with self.lock:
            self.val.value = value

class Counter(object):
    def __init__(self, init_val=0):
        self.val = Value('i', init_val)
        self.init_val = init_val
        self.lock = Lock()

    def increment(self):
        with self.lock:
            self.val.value += 1

    def value(self):
        with self.lock:
            return self.val.value
        
    def reset(self):
        with self.lock:
            self.val.value = self.init_val

class AtomicInt(object):
    def __init__(self, initval=-1):
        self.val = Value('i', initval)
        self.lock = Lock()

    def value(self):
        with self.lock:
            return self.val.value
        
    def reset(self):
        with self.lock:
            self.val.value = -1

    def set_value(self, value):
        with self.lock:
            self.val.value = value

class MPSWeights(Sequence):
    def __init__(self, num_processes, increment=2, update_interval=5, log_path=None):
        self.weights = Array('i', [10]*num_processes)
        self.num_processes = num_processes
        self.percentages = Array('i', [0]*num_processes)
        self.set_percentages()
        self.increment = increment
        self.iterations = Counter(0) # Number of iterations algorithm have run for
        self.update_interval = update_interval
        self.converged = AtomicInt(0) # Whether or not we've converged
        self.updates = Counter(0) # Amount of updates done in one iteration 
        self.biggest_queue = MaxVal(0) # Value of the biggest queue
        self.biggest_index = AtomicInt(-1) # Index of process with the biggest queue size
        self.equals = Counter(1) # Amount of queue sizes that are equal
        self.convergence_counter = Counter(0) # Number of times queues across training processes have been equal
        if log_path:
            self.logger = MPSLogger(log_path)
        super().__init__()

    def set_percentages(self):
        total = sum(self.weights)
        for idx, w in enumerate(self.weights):
            self.percentages[idx] = int(w/total*100)

    def increment_weight(self, i):
        self.weights[i] += self.increment
        self.set_percentages()

    def __getitem__(self, i):
        return self.percentages[i]
    def __len__(self):
        return len(self.percentages)
    
    def set_convergence(self):
        self.converged.set_value(1)
    
    def get_convergence(self):
        return self.converged.value()

    def update_weights(self, pid):

        # If queue sizes for all training processes have been updated in current iteration, we can update weights
        if self.updates.value() == self.num_processes:

            # Queue sizes across training processes were too similar, updating one over another would skew training speed
            if self.equals.value() == self.num_processes:

                self.convergence_counter.increment()
                self.equals.reset()
                self.biggest_queue.set_value(0)
                self.biggest_index.reset()

                # If we've seen similar queue sizes across training processes three times in a row, assume convergence
                if self.convergence_counter.value() == 3:
                    print("Weights were not updated for three iterations, setting convergence")
                    self.set_convergence()
            
            # Queue size of some training process was greater than others, increment weight for that process
            else:
                if self.biggest_index.value() > -1:
                    self.increment_weight(self.biggest_index.value())
                    self.biggest_queue.set_value(0)

                self.logger.write_line(str(pid),str(self.iterations.value()),"","","",
                                       str(self.biggest_index.value()),"","",str(self.convergence_counter.value()))
                
                if self.biggest_index.value() > -1:
                    self.biggest_index.reset()
                self.equals.reset()
                self.convergence_counter.reset()

            self.updates.reset()
            self.iterations.increment()
        

    def set_qsize(self, qsize, i, pid, model_name):

        self.logger.write_line(str(pid),str(self.iterations.value()),str(i),model_name,str(qsize),"",
                               str(self.weights[i]),str(self.percentages[i]),str(self.convergence_counter.value()))

        # Check if queue sizes were roughly the same and that both biggest queue size recorded and current queue size is not 0
        if abs(qsize - self.biggest_queue.value()) <= 2 and (self.biggest_queue.value() != 0 and qsize != 0):
            # If above is the case, we deem the queues equal and increment towards convergence
            self.equals.increment()
        elif qsize > self.biggest_queue.value():
            # Otherwise, set process with biggest queue size to have weight updated next
            self.biggest_queue.set_value(qsize)
            self.biggest_index.set_value(i)
        self.updates.increment()
        

class MyQueue(object):
    def __init__(self, queue, index):
        self.queue = queue
        self.index = index

# TODO: Make this a shared object: https://stackoverflow.com/questions/3671666/sharing-a-complex-object-between-processes
class Logger(object):
    def __init__(self, args, pid=0, log_path=None, gpu_path=None):
        self.args = args
        self.pid = pid
        self.log_path = log_path
        self.gpu_path = gpu_path
        
        self.train_time = 0
        self.batch_time = 0
        self.val_acc = 0
        self.val_loss = 0
        self.val_correct = 0
        self.val_time = 0

        self.mps_train_time = 0
        self.mps_batch_time = 0
        self.mps_misc_time = 0 # TODO: Instead of rolling this into train time, save in distinct column?

    def set_mps_time(self, mps_time):
        self.mps_train_time = mps_time["train_time"]
        self.mps_batch_time = mps_time["batch_time"]
        self.mps_misc_time = mps_time["misc_time"]
    
    def log_train_interval(self, idx, epoch, num_items, loss, items_processed, train_time, batch_time):
        self.train_time = train_time
        self.batch_time = batch_time

        if idx % self.args.log_interval == 0:
            print('{}\tTrain Epoch: {} [{}/{} ({:.0f}%)] Loss: {:.2f} Throughput [img/s]: {:.1f}'.format(
                self.pid, epoch, idx * num_items, self.args.train_dataset_len,
                100. * idx / self.args.train_loader_len, loss.item(), items_processed/(train_time+batch_time)))

    def log_validation(self, val_loss, val_correct, val_acc, val_time):
        self.val_time = val_time
        self.val_acc = val_acc
        self.val_loss = val_loss
        self.val_correct = val_correct
        
    def log_write_epoch_end(self, epoch, epoch_time, train_acc, train_running_corrects):
        # If we have done some MPS-weight finding, we need to include the train and batch time from that in the total epoch time
        if any((self.mps_train_time, self.mps_batch_time, self.mps_misc_time)):
            print(f"Adding time from MPS finding: {self.mps_train_time + self.mps_batch_time + self.mps_misc_time}")
            self.train_time += (self.mps_train_time + self.mps_misc_time)
            self.val_time += self.mps_batch_time
            epoch_time += (self.mps_train_time + self.mps_batch_time + self.mps_misc_time)
            self.mps_train_time, self.mps_batch_time, self.mps_misc_time = 0,0,0

        print('{} Validation: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\n'.format(
            self.pid, self.val_loss, self.val_correct, self.args.valid_dataset_len, self.val_acc))
        
        print(f"{self.pid} Epoch {epoch} end: {round(epoch_time,1)}s, Train accuracy: {round(train_acc,2)}")
        if self.args.log_dir:
            with open(self.log_path, "a") as f:
                f.write(f"{epoch},{train_acc},{self.val_acc},{self.train_time},{self.batch_time},{self.val_time},{epoch_time},{train_running_corrects},{self.val_correct}\n")
                os.system(f"nvidia-smi --query-compute-apps=gpu_uuid,pid,used_memory --format=csv,noheader >> {self.gpu_path}")

        self.train_time = 0
        self.batch_time = 0

class MPSLogger(object):
    def __init__(self, log_path=None):
        self.log_path = log_path
        self.log_path.parent.mkdir(parents=True, exist_ok=True)
        with open(self.log_path, "w") as f:
            f.write("pid,iteration,index,model,qsize,incremented_index,weight,thread_percentage,convergence_count\n")

    def write_line(self, *args):
        line = ",".join(args)
        with open(self.log_path, "a") as f:
            f.write(f"{line}\n")


# Use this worker for setting MPS weights
def MPS_worker(q, model, mps_weights, args, mps_time):
    alive = mp.Event()
    alive.set()
    proc_id = os.getpid()

    if model.on_device == False:
        model.send_model()
        model.scheduler.step()
    
    convergence = False

    train_time, val_time, batch_time, items_processed = 0,0,0,0
    # Process can be terminated if we've converged
    while alive.is_set():
        start = time.time()
        idx, inputs, labels, epoch, batch_type, indices = q.queue.get()
        
        # Log batch time as MPS time to be included in actual training logs
        mps_time["batch_time"] += time.time() - start
        #batch_time = time.time() - start
        
        # If:
        #   1) We haven't converged (seen three iterations with (roughly) equal queue sizes)
        #   2) We have gone through N batches, where N corresponds to the update_interval
        #   3) We will not be restarting another of these processes afterwards
        # then we can update MPS weights
        if not convergence and idx % mps_weights.update_interval == 0:
            mps_weights.set_qsize(q.queue.qsize(), q.index, proc_id, model.name)
            mps_weights.update_weights(proc_id)

            convergence = mps_weights.get_convergence()

            # Terminate MPS weight-searching process
            alive.clear()

        if batch_type == "train":
            start = time.time()
            loss = model.forward(inputs, labels)
            items_processed += len(inputs)

            # Log training time as MPS time to be included in actual training logs
            mps_time["train_time"] += time.time() - start
            #train_time = time.time() - start
            #logger.log_train_interval(idx, epoch, len(inputs), loss, items_processed, train_time, batch_time, set_mps_time=True)

        # For now, hope we dont reach this stage :D
        elif batch_type == "valid":
            val_loss, val_acc, val_correct = model.validate(inputs, labels)
            
        elif batch_type == "end":
            train_epoch_acc, train_running_corrects = model.end_epoch(args)
        
        q.queue.task_done()

def worker(q, model, args, mps_time=None):
    log_path, gpu_path, pid = None, None, 0
    if model.on_device == False:
        pid = os.getpid()
        print(f"{pid}\tTraining model: {model.name}")
        model.send_model()
        model.scheduler.step()
        if args.log_dir:
            log_path, gpu_path = model.init_log(pid)
    #if not logger:
    #    print("Creating own logger")
    logger = Logger(args, pid, log_path, gpu_path)
    if mps_time:
        logger.set_mps_time(mps_time)
    #if logger and log_path and gpu_path and pid:
    #    logger.set_pid(pid)
    #    logger.set_paths(log_path, gpu_path)

    if not args.record_first_batch_time:
        print("Skipping recording batch time for first batch!")
    
    debug_indices_path, debug_indices_val_path = None, None

    train_time, val_time, batch_time, items_processed = 0,0,0,0
    big_start = time.time()
    while True:
        start = time.time()
        idx, inputs, labels, epoch, batch_type, indices = q.queue.get()

        if args.debug_data_dir:
            debug_indices_path = Path(args.debug_data_dir) / model.name / f"epoch_{epoch}" / "indices.txt"
            debug_indices_path.parent.mkdir(parents=True, exist_ok=True)
            debug_indices_val_path = Path(args.debug_data_dir) / model.name / f"epoch_{epoch}" / "val_indices.txt"
            debug_indices_val_path.parent.mkdir(parents=True, exist_ok=True)

        if args.record_first_batch_time:
            batch_time += time.time() - start
        else:
            if idx > 0:
                batch_time += time.time() - start

        start = time.time()
        if batch_type == "train":
            write_debug_indices(indices, debug_indices_path, args)
            loss = model.forward(inputs, labels)
            items_processed += len(inputs)
            train_time += time.time() - start
            logger.log_train_interval(idx, epoch, len(inputs), loss, items_processed, train_time, batch_time)

        elif batch_type == "valid":
            write_debug_indices(indices, debug_indices_val_path, args)
            val_loss, val_acc, val_correct = model.validate(inputs, labels)
            val_time += time.time() - start
            logger.log_validation(val_loss, val_correct, val_acc, val_time)
            
        elif batch_type == "end":
            train_epoch_acc, train_running_corrects = model.end_epoch(args)
            epoch_time = train_time + val_time + batch_time
            logger.log_write_epoch_end(epoch, epoch_time, train_epoch_acc, train_running_corrects)
            train_time, val_time, batch_time,items_processed = 0,0,0,0
            print(f"Epoch took {time.time() - big_start} seconds")

        q.queue.task_done()

if __name__ == "__main__":

    args = parser.parse_args()
    assert args.num_processes == len(args.arch)

    if args.seed is not None:
        print(f"Setting seed {args.seed}")
        random.seed(args.seed)
        torch.manual_seed(args.seed)
        cudnn.deterministic = True
        cudnn.benchmark = False
        print('You have chosen to seed training. '
                      'This will turn on the CUDNN deterministic setting, '
                      'which can slow down your training considerably!')
        
    mp.set_start_method("spawn", force=True)
    manager = mp.Manager()
    mps_time = manager.dict()
    mps_time["train_time"] = 0
    mps_time["batch_time"] = 0

    """Initialise dataset"""
    labels = ads3.get_labels()

    pretrain = ["imagenet" if p == "true" else None for p in args.pretrained]

    device = torch.device("cuda")

    dataset = CarDataset(file_path=file_train, folder_images=folder_images, labels=labels)

    train_models = []
    for idx, arch in enumerate(args.arch):
        model = torchvision.models.__dict__[arch](pretrained=pretrain[idx])
        model.name = arch + "_pretrained" if pretrain[idx] else arch
        print(f"Model: {model.name}")
        proc_model = ProcTrainer(args, model, device)
        train_models.append(proc_model)

    queues = []
    for idx in range(args.num_processes):
        q = JoinableQueue(maxsize=10)
        queue = MyQueue(q, idx)
        queues.append(queue)

    mps_log_path = Path("/home/kafka/repos/thesis/mps_log.csv")
    
    mps_weights = MPSWeights(args.num_processes, update_interval=10, log_path=mps_log_path)

    """Split train and test"""
    train_len = int(0.7 * len(dataset))
    valid_len = len(dataset) - train_len
    train_set, valid_set = D.random_split(dataset, lengths=[train_len, valid_len], generator=torch.Generator().manual_seed(42))

    train_set = DatasetFromSubset(train_set, train_transforms)
    valid_set = DatasetFromSubset(valid_set, valid_transforms)

    if args.dummy_data:
        print("=> Dummy CompCars data is used!")
        train_dataset = datasets.FakeData(11336, (3, 224, 224), 431, train_transforms)
        val_dataset = datasets.FakeData(4680, (3, 224, 224), 431, valid_transforms)

    train_loader = D.DataLoader(
        train_set,
        batch_size=args.batch_size,
        shuffle=True,
        num_workers=args.training_workers,
        pin_memory=False,
        prefetch_factor=args.prefetch_factor,
    )

    valid_loader = D.DataLoader(
        valid_set,
        batch_size=args.batch_size,
        shuffle=True,
        num_workers=args.validation_workers,
        pin_memory=True,
        prefetch_factor=args.prefetch_factor,
    )

    args.train_dataset_len = len(train_loader.dataset)
    args.train_loader_len = len(train_loader)

    args.valid_dataset_len = len(valid_loader.dataset)
    args.valid_loader_len = len(valid_loader)

    producers = []
    for i in range(1):
        p = Process(target=producer, args = ((train_loader, valid_loader, queues, device, args)))
        producers.append(p)
        p.start()

    # Measuring time for entire training
    _start = time.time()

    restart = True
    # Measuring time spent on MPS stuff only
    mps_start =time.time()

    # Keep starting MPS-weight searching training processes until we have converged at satisfactory MPS weights
    print("Started finding MPS thread weights...")
    
    logger = Logger(args)
    while restart:
        workers = []
        # Start training processes with default MPS percentages
        for i in range(args.num_processes):
            os.environ['CUDA_MPS_ACTIVE_THREAD_PERCENTAGE'] = str(mps_weights[i])
            p = Process(target=MPS_worker, daemon=True, args=((queues[i], train_models[i], mps_weights, args, mps_time)))
            workers.append(p)
            p.start()

        # Weight-search converged, stop restarting new processes
        if mps_weights.get_convergence():
            restart = False

        for w in workers:
            w.join()
    
    total_mps_time = time.time() - mps_start
    mps_time["misc_time"] = total_mps_time - sum(mps_time.values())
    print(f"Done with figuring out MPS weights, took {total_mps_time} seconds")
    print(f"Spent {mps_time['train_time']} seconds on training, {mps_time['batch_time']} seconds on batch time and {mps_time['misc_time']} seconds on misc MPS stuff")

    workers = []
    for i in range(args.num_processes):
        print(f"Setting MPS threads to {mps_weights[i]}")
        os.environ['CUDA_MPS_ACTIVE_THREAD_PERCENTAGE'] = str(mps_weights[i])
        p = Process(target=worker, daemon=True, args=((queues[i], train_models[i], args, mps_time)))
        workers.append(p)
        p.start()

    for p in producers:
        print("Waiting for producer finish ...")
        p.join()

    for q in queues:
        q.queue.join()
    print(f"Completed in {time.time() - _start} seconds")