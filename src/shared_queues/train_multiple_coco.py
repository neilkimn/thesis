import torch
import torch.backends.cudnn as cudnn
import torchvision
import torchvision.transforms as transforms
import torchvision.models as models
import torchvision.datasets as datasets

import torch.multiprocessing as mp
from torch.multiprocessing import Process, Queue, JoinableQueue
from torch.utils import data as D
from torch.autograd import Variable
import nvtx

from pathlib import Path
import shutil
import argparse
import random
import time
import os

from shared.dataset import CarDataset, DatasetFromSubset
from shared_queues.trainer import RCNNProcTrainer
from shared.util import Counter, get_transformations, write_debug_indices

INPUT_SIZE = 224
data_path = Path(os.environ["DATA_PATH"])

from PIL import ImageFile
ImageFile.LOAD_TRUNCATED_IMAGES = True

model_names = sorted(name for name in models.__dict__
    if name.islower() and not name.startswith("__")
    and callable(models.__dict__[name]))
my_datasets = ["compcars", "imagenet", "imagenet64x64"]

parser = argparse.ArgumentParser()
parser.add_argument('--batch-size', type=int, default=80)
parser.add_argument('--num-processes', type=int, default=2)
parser.add_argument('--training-workers', type=int, default=1)
parser.add_argument('--validation-workers', type=int, default=1)
parser.add_argument('--prefetch-factor', type=int, default=1)
parser.add_argument('--seed', type=int, default=None)
parser.add_argument('--epochs', type=int, default=5)
parser.add_argument('--use-dali', action='store_true', help="whether to use DALI for data-loading")
parser.add_argument('--log-interval', type=int, default=10, metavar='N',
                    help='how many batches to wait before logging training status')
parser.add_argument('-a','--arch', nargs='+', metavar='ARCH', help='model architecture: ' +
                        ' | '.join(model_names) +
                        ' (default: resnet18)', default='resnet18')
parser.add_argument('--dataset', metavar='DATASET', help='dataset: ' +
                        ' | '.join(my_datasets) +
                        ' (default: compcars)', default='compcars')

parser.add_argument('--debug_data_dir', metavar='DIR', nargs='?', default='',
                    help='path to store data generated by dataloader')
parser.add_argument('--overwrite_debug_data', type=int, default=1)
parser.add_argument('--log_dir', metavar='LOG_DIR', nargs='?', default='',
                    help='path to store training log')
parser.add_argument('--pretrained', nargs='+', metavar="PRETRAIN", help="Whether to pretrain a certain model")
parser.add_argument('--dummy-data', action='store_true', help="use fake data to benchmark")
parser.add_argument('--record_first_batch_time', action='store_true', help="Don't skip measuring time spent on first batch")

parser.add_argument(
        '-c', '--config', default=None,
        help='path to the data config file'
    )
parser.add_argument(
        '-m', '--model', default='fasterrcnn_resnet50_fpn',
        help='name of the model'
    )
parser.add_argument(
        '-d', '--device', 
        default=torch.device('cuda:0' if torch.cuda.is_available() else 'cpu'),
        help='computation/training device, default is GPU if GPU present'
    )
parser.add_argument(
        '-ims', '--img-size', dest='img_size', default=640, type=int, 
        help='image size to feed to the network'
    )
parser.add_argument(
        '-w', '--weights', default=None, type=str,
        help='path to model weights if using pretrained weights'
    )

def dali_producer(qs, device, args, producer_alive):
    from dali_dataset import DALIDataset
    pid = os.getpid()
    if args.seed:
        torch.manual_seed(args.seed)
    if args.debug_data_dir:
        if args.overwrite_debug_data:
            shutil.rmtree(args.debug_data_dir)
    debug_indices_path, debug_indices_val_path = None, None

    if args.dataset == "compcars":
        images_train = data_path / args.dataset / "image_train"
        images_valid = data_path / args.dataset / "image_valid"
    if args.dataset in ("imagenet", "imagenet64x64", "imagenet_10pct"):
        images_train = data_path / args.dataset / "train"
        images_valid = data_path / args.dataset / "val"
        INPUT_SIZE = 224
    if args.dataset == "cifar10":
        images_train = data_path / args.dataset / "train"
        images_valid = data_path / args.dataset / "test"
        INPUT_SIZE = 32

    train_loader = DALIDataset(args.dataset, images_train, args.batch_size, args.training_workers, INPUT_SIZE)
    valid_loader = DALIDataset(args.dataset, images_valid, args.batch_size, args.validation_workers, INPUT_SIZE)
    
    args.train_dataset_len = len(train_loader)
    args.train_loader_len = len(train_loader)
    args.valid_dataset_len = len(valid_loader)
    args.valid_loader_len = len(valid_loader)

    for epoch in range(1, args.epochs+1):
        if args.debug_data_dir:
            debug_indices_path = Path(args.debug_data_dir) / f"epoch_{epoch}" / f"{pid}_producer_indices.txt"
            debug_indices_path.parent.mkdir(parents=True, exist_ok=True)

        for idx, data in enumerate(train_loader.dataset):
            inputs, labels = data[0]["data"], data[0]["label"]
            indices = None
            labels = labels.long()
            inputs = Variable(inputs.to(device))
            labels = Variable(labels.to(device))
            for q in qs:
                q.queue.put((idx, inputs, labels, epoch, "train", indices))
            
            #write_debug_indices(indices, debug_indices_path, args)

        # end of training for epoch, switch to eval
        if epoch > 10:
            if args.debug_data_dir:
                debug_indices_val_path = Path(args.debug_data_dir) / f"epoch_{epoch}" / f"{pid}_producer_val_indices.txt"
                debug_indices_val_path.parent.mkdir(parents=True, exist_ok=True)

            for idx, data in enumerate(valid_loader.dataset):
                inputs, labels = data[0]["data"], data[0]["label"]
                indices = None
                labels = labels.long()
                inputs = Variable(inputs.to(device))
                labels = Variable(labels.to(device))

                for q in qs:
                    q.queue.put((idx, inputs, labels, epoch, "valid", indices))

                #write_debug_indices(indices, debug_indices_val_path, args)
        
        for q in qs:
            q.queue.put((0, None, None, epoch, "end", None))
    producer_alive.wait()

from torch_utils import utils

def producer(loader, valid_loader, qs, device, args, producer_alive):
    pid = os.getpid()
    if args.seed:
        torch.manual_seed(args.seed)
    if args.debug_data_dir:
        if args.overwrite_debug_data:
            shutil.rmtree(args.debug_data_dir)
    debug_indices_path, debug_indices_val_path = None, None
    metric_logger = utils.MetricLogger(delimiter="  ")
    #metric_logger.add_meter("lr", utils.SmoothedValue(window_size=1, fmt="{value:.6f}"))
    print_freq = 100

    for epoch in range(1, args.epochs+1):
        header = f"Epoch: [{epoch}]"
        if args.debug_data_dir:
            debug_indices_path = Path(args.debug_data_dir) / f"epoch_{epoch}" / f"{pid}_producer_indices.txt"
            debug_indices_path.parent.mkdir(parents=True, exist_ok=True)

        nvtx.push_range("Start batch")
        for idx, (images, targets) in enumerate(metric_logger.log_every(loader, print_freq, header)):
            nvtx.pop_range()
            indices = None
            nvtx.push_range("memcpy")
            images = list(image.to(device) for image in images)
            targets = [{k: v.to(device) for k, v in t.items()} for t in targets]
            nvtx.pop_range()
            for q in qs:
                q.queue.put((idx, images, targets, epoch, "train", indices))

            metric_logger.update()
            nvtx.push_range("Start batch")
            #write_debug_indices(indices, debug_indices_path, args)

        # end of training for epoch, switch to eval
        if epoch > 2:
            if args.debug_data_dir:
                debug_indices_val_path = Path(args.debug_data_dir) / f"epoch_{epoch}" / f"{pid}_producer_val_indices.txt"
                debug_indices_val_path.parent.mkdir(parents=True, exist_ok=True)

            for idx, (inputs, labels) in enumerate(valid_loader):
                indices = None
                nvtx.push_range("memcpy")
                images = list(image.to(device) for image in images)
                targets = [{k: v.to(device) for k, v in t.items()} for t in targets]
                nvtx.pop_range()

                for q in qs:
                    q.queue.put((idx, inputs, labels, epoch, "valid", indices))

                #write_debug_indices(indices, debug_indices_val_path, args)
        
        for q in qs:
            q.queue.put((0, None, None, epoch, "end", None))
    producer_alive.wait()



class MyQueue(object):
    def __init__(self, queue, index):
        self.queue = queue
        self.index = index

class Logger(object):
    def __init__(self, args, pid, log_path=None, gpu_path=None):
        self.args = args
        self.pid = pid
        self.log_path = log_path
        self.gpu_path = gpu_path
        
        self.train_time = 0
        self.batch_time = 0
        self.val_acc = 0
        self.val_loss = 0
        self.val_correct = 0
        self.val_time = 0
    
    def log_train_interval(self, idx, epoch, num_items, loss, items_processed, train_time, batch_time):
        self.train_time = train_time
        self.batch_time = batch_time

        if idx % self.args.log_interval == 0:
            print('{}\tTrain Epoch: {} [{}/{} ({:.0f}%)] Loss: {:.2f} Throughput [img/s]: {:.1f}'.format(
                self.pid, epoch, idx * num_items, self.args.train_dataset_len,
                100. * idx / self.args.train_loader_len, loss, items_processed/(train_time+batch_time)))

    def log_validation(self, val_loss, val_correct, val_acc, val_time):
        self.val_time = val_time
        self.val_acc = val_acc
        self.val_loss = val_loss
        self.val_correct = val_correct
        
    def log_write_epoch_end(self, epoch, epoch_time, train_acc, train_running_corrects):
        #print('{} Validation: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\n'.format(
        #    self.pid, self.val_loss, self.val_correct, self.args.valid_dataset_len, self.val_acc))
        
        print(f"{self.pid} Epoch {epoch} end: {round(epoch_time,1)}s, Train accuracy: {round(train_acc,2)}")
        if self.args.log_dir:
            with open(self.log_path, "a") as f:
                f.write(f"{int(time.time())},{epoch},{train_acc},{self.val_acc},{self.train_time},{self.batch_time},{self.val_time},{epoch_time},{train_running_corrects},{self.val_correct}\n")
                os.system(f"nvidia-smi --query-compute-apps=gpu_uuid,pid,used_memory --format=csv,noheader >> {self.gpu_path}")

import math
import sys

def worker(q, model, args, producer_alive, finished_workers):
    pid = os.getpid()
    log_path, gpu_path = None, None
    if model.on_device == False:
        print(f"{pid}\tTraining model: {args.model}")
        model.send_model()
        model.scheduler.step()
        if args.log_dir:
            log_path, gpu_path = model.init_log(pid)
    
    logger = Logger(args, pid, log_path, gpu_path)    
    if not args.record_first_batch_time:
        print("Skipping recording batch time for first batch!")
    
    debug_indices_path, debug_indices_val_path = None, None

    epochs_processed = 0

    train_time, val_time, batch_time, items_processed = 0,0,0,0
    while True:
        pid = os.getpid()
        
        start = time.time()
        nvtx.push_range("Start get batch")
        idx, images, targets, epoch, batch_type, indices = q.queue.get()
        #images = list(image.to(args.device) for image in images)
        #targets = [{k: v.to(args.device) for k, v in t.items()} for t in targets]
        nvtx.pop_range()

        if args.debug_data_dir:
            debug_indices_path = Path(args.debug_data_dir) / args.model / f"{pid}_epoch_{epoch}" / "indices.txt"
            debug_indices_path.parent.mkdir(parents=True, exist_ok=True)
            debug_indices_val_path = Path(args.debug_data_dir) / args.model / f"{pid}_epoch_{epoch}" / "val_indices.txt"
            debug_indices_val_path.parent.mkdir(parents=True, exist_ok=True)

        if args.record_first_batch_time:
            batch_time += time.time() - start
        else:
            if idx > 0:
                batch_time += time.time() - start

        if idx % 100 == 0:
            print(f"Memory allocated (MB): {torch.cuda.max_memory_allocated() / (1024.0 * 1024.0)}")

        start = time.time()
        if batch_type == "train":
            #write_debug_indices(indices, debug_indices_path, args)

            nvtx.push_range("model forward")
            loss_dict = model.forward(images, targets)
            nvtx.pop_range()

            nvtx.push_range("reduce losses")
            loss_dict_reduced = utils.reduce_dict(loss_dict)
            losses_reduced = sum(loss for loss in loss_dict_reduced.values())
            loss_value = losses_reduced.item()
            nvtx.pop_range()

            if not math.isfinite(loss_value):
                print(f"Loss is {loss_value}, stopping training")
                print(loss_dict_reduced)
                sys.exit(1)

            nvtx.push_range("optimizer zero grad")
            model.optimizer.zero_grad()
            nvtx.pop_range()

            losses = sum(loss for loss in loss_dict.values())

            nvtx.push_range("loss backward")
            losses.backward()
            nvtx.pop_range()

            nvtx.push_range("optimizer step")
            model.optimizer.step()
            nvtx.pop_range()

            

            train_time += time.time() - start
            #logger.log_train_interval(idx, epoch, len(images), loss_value, items_processed, train_time, batch_time)

        #elif batch_type == "valid":
            #write_debug_indices(indices, debug_indices_val_path, args)
            #inputs = Variable(inputs.to(args.device))
            #labels = Variable(labels.to(args.device))

            #val_loss, val_acc, val_correct = model.validate(inputs, labels)
            #val_time += time.time() - start
            #logger.log_validation(val_loss, val_correct, val_acc, val_time)
            
        elif batch_type == "end":
            train_epoch_acc, train_running_corrects = model.end_epoch(args)
            epoch_time = train_time + val_time + batch_time
            logger.log_write_epoch_end(epoch, epoch_time, train_epoch_acc, train_running_corrects)
            train_time, val_time, batch_time,items_processed = 0,0,0,0

        q.queue.task_done()

        if batch_type == "end":
            epochs_processed += 1
            if epochs_processed == args.epochs:
                finished_workers.increment()
                if finished_workers.value() == args.num_processes:
                    producer_alive.set()
                break

import yaml
import numpy as np
from torch_utils.coco_utils import get_coco
from models.create_fasterrcnn_model import create_model

def collate_fn(batch):
    """
    To handle the data loading as different images may have different number 
    of objects and to handle varying size tensors as well.
    """
    return tuple(zip(*batch))

def create_train_loader(train_dataset, batch_size, num_workers=8):
    train_loader = D.DataLoader(
        train_dataset,
        batch_size=batch_size,
        shuffle=True,
        num_workers=num_workers,
        collate_fn=collate_fn
    )
    return train_loader
def create_valid_loader(valid_dataset, batch_size, num_workers=8):
    valid_loader = D.DataLoader(
        valid_dataset,
        batch_size=batch_size,
        shuffle=False,
        num_workers=num_workers,
        collate_fn=collate_fn
    )
    return valid_loader

if __name__ == "__main__":

    args = parser.parse_args()
    with open(args.config) as file:
        data_configs = yaml.safe_load(file)

    TRAIN_DIR_IMAGES = data_configs['TRAIN_DIR_IMAGES']
    TRAIN_DIR_LABELS = data_configs['TRAIN_DIR_LABELS']
    VALID_DIR_IMAGES = data_configs['VALID_DIR_IMAGES']
    VALID_DIR_LABELS = data_configs['VALID_DIR_LABELS']
    CLASSES = data_configs['CLASSES']
    NUM_CLASSES = data_configs['NC']
    NUM_WORKERS = args.training_workers
    DEVICE = args.device
    NUM_EPOCHS = 1
    BATCH_SIZE = args.batch_size
    COLORS = np.random.uniform(0, 1, size=(len(CLASSES), 3))

    IMAGE_WIDTH = args.img_size
    IMAGE_HEIGHT = args.img_size

    train_dataset = get_coco("/home/neni/datasets/coco_minitrain_25k", "train", None)
    valid_dataset = get_coco("/home/neni/datasets/coco_minitrain_25k", "val", None)
    train_loader = create_train_loader(train_dataset, BATCH_SIZE, NUM_WORKERS)
    valid_loader = create_valid_loader(valid_dataset, BATCH_SIZE, NUM_WORKERS)
    print(f"Number of training samples: {len(train_dataset)}")
    print(f"Number of validation samples: {len(valid_dataset)}\n")

    if args.seed is not None:
        print(f"Setting seed {args.seed}")
        random.seed(args.seed)
        torch.manual_seed(args.seed)
        cudnn.deterministic = True
        cudnn.benchmark = False
        print('You have chosen to seed training. '
                      'This will turn on the CUDNN deterministic setting, '
                      'which can slow down your training considerably!')
        
    mp.set_start_method("spawn", force=True)
    manager = mp.Manager()


    pretrain = ["imagenet" if p == "true" else None for p in args.pretrained]

    device = torch.device("cuda")


    train_models = []
    for idx, arch in enumerate(["1", "2",]):
        if args.weights is None:
            print('Building model from scratch...')
            build_model = create_model[args.model]
            model = build_model(num_classes=NUM_CLASSES, pretrained=True, coco_model=True)
        name = args.model
        print(f"Model: {args.model}")
        proc_model = RCNNProcTrainer(args, model, device, name)
        train_models.append(proc_model)

    queues = []
    for idx in range(args.num_processes):
        q = JoinableQueue(maxsize=20)
        queue = MyQueue(q, idx)
        queues.append(queue)

    _start = time.time()

    producer_alive = mp.Event()
    producers = []
    if not args.use_dali:
        for i in range(1):
            train_loader = train_loader
            
            valid_loader = valid_loader

            args.train_dataset_len = 25000
            args.train_loader_len = 25000 / BATCH_SIZE

            p = Process(target=producer, args = ((train_loader, valid_loader, queues, device, args, producer_alive)))
            #p = Process(target=producer, args = ((train_loader, valid_loader, [queues[i]], device, args, producer_alive)))
            producers.append(p)
            p.start()
    else:
        from dali_dataset import DALIDataset
        for i in range(1):
            p = Process(target=dali_producer, args = ((queues, device, args, producer_alive)))
            producers.append(p)
            p.start()

    args.device = device
    finished_workers = Counter(0)
    workers = []
    for i in range(args.num_processes):
        p = Process(target=worker, daemon=True, args=((queues[i], train_models[i], args, producer_alive, finished_workers)))
        workers.append(p)
        p.start()

    for p in workers:
        p.join()

    print(f"Completed in {time.time() - _start} seconds")